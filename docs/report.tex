\documentclass[a4paper,12pt]{article}

% Include packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{minted}

% Set the page margins
\geometry{top=1in, bottom=1in, left=1in, right=1in}

% Fancy header
\pagestyle{fancy}
\setlength{\headheight}{14.49998pt}
\fancyhead[L]{CDCL Solver Report}
\fancyhead[R]{Date}

\title{ABCDCL - an implementation of CDCL with two watched literals in Rust\\
{\footnotesize Course Project of DCC831 Theory and Practice of SMT Solving}
}
\author{Atila Carvalho \& Bernardo Borges}
\date{January 27, 2025}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}

This report presents our course project for \textit{DCC831 - Theory and Practice of SMT Solving},
where we implemented and evaluated a modern SAT solver based on the Conflict-Driven Clause Learning
(CDCL) algorithm \cite{CdclAlgorithm}. Through this implementation, we explored both the theoretical
foundations and practical challenges of building an efficient SAT solver, gaining valuable insights
into the complexities of state-of-the-art approaches.

The CDCL algorithm incorporates several key concepts covered in the course, such as unit propagation,
clause learning, and non-chronological backtracking. Understanding these principles was essential not
only for implementing the solver but also for debugging and refining its performance. Throughout the
development process, we wrote extensive tests to verify correctness and experimented with different
heuristics to optimize decision-making.

We chose Rust \cite{RustLang} as our implementation language due to its strong emphasis on memory
safety, performance, and modern software engineering practices. Additionally, Rust is the primary
language used in \textit{Carcara} \cite{Carcara}, a proof checker we have been working with, making
it a natural choice for this project.

To evaluate our solver, we developed scripts that automatically download SAT benchmarks \cite{SatBenchmarks}
in the DIMACS format and run them against both our implementation and the widely used \textit{MiniSat}
solver \cite{Minisat}. The results are logged and compared, providing an empirical analysis of our
solver's performance and correctness.

% While we faced numerous challenges throughout this project, including algorithmic misunderstandings
% and debugging complexities, this experience has been highly educational. In the following sections,
% we describe our implementation, the difficulties encountered, the lessons learned, and the experimental evaluation of our solver.

% \section{Introduction}
% \label{sec:introduction}
% This is the course project of "DCC831 Theory and Practice of SMT Solving"
% In here we had the opportunity to see in practice the implementation insights and
% difficulties of an state-of-the-art sat solver.
% The CDCL algorithm takes on many of the concepts learnt in this course, which were paramount
% for implementing and debugging, features and tests.
% We used rust for it's modern, memory safe and object of our work at Carcara proof checker.
% We had many difficulties that will be shown but this was a major learning experience.
% We also created some scripts that automatically download benchmarks in the DIMACS format and
% run our project against the result of minisat, logging the results.

\section{Solver Design and Implementation}
\label{sec:design}
% TODO
This section should describe the architecture and design choices of your CDCL solver. Some topics to cover:
\begin{itemize}
    \item Description of the CDCL algorithm: search process, conflict analysis, backtracking, clause learning.
    \item Key data structures used in the solver (e.g., decision stack, implication graph, learned clauses).
    \item Heuristics implemented (e.g., variable selection, decision ordering).
    \item Optimizations applied (e.g., restarts, watched literals, UIP learning).
\end{itemize}


\section{Testing and Evaluation}

For the evaluation of our CDCL solver, we used a set of benchmark problems in DIMACS
format from the SATLIB repository. Our solver was compared against MiniSat, a
well-established SAT solver, in terms of correctness and runtime.

\subsection{Experimental Setup}
All experiments were conducted on a laptop with the following hardware and software configuration:

\begin{itemize}
    \item \textbf{Operating System:} Arch Linux (Kernel 6.12.10)
    \item \textbf{Host:} ASUS VivoBook X512FJ
    \item \textbf{Processor:} Intel Core i7-8565U (8 threads, up to 4.60GHz)
    \item \textbf{RAM:} 8GB (usable: 7.8GB)
    \item \textbf{GPU:} Intel UHD Graphics 620 (Whiskey Lake) + NVIDIA GeForce MX230
\end{itemize}

\subsection{Results}
For each benchmark instance, we recorded the solver’s result (SAT or UNSAT) and its
execution time in milliseconds. The results were then compared against MiniSat to
verify correctness and measure performance. The summarized results are presented in
Table~\ref{tab:results}, comparing Avg Time to solve in \textit{ms} for each benchmark
and it's correctness overall.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Benchmark} & \textbf{ABCDCL Time} & \textbf{MiniSat Time} & \textbf{Results } \\
        \hline
        % TODO
        jnh                & 397                  & 0                     & 50 / 0 / 0 / 50   \\
        jnh                & 492                  & 2                     & 50 / 0 / 0 / 50   \\

        \hline
    \end{tabular}
    \caption{Performance comparison of ABCDCL and MiniSat across benchmark families. Results is a
        counter of \texttt{(PASS/FAIL/TIME/TOTAL)}. The timeout used was $60$ seconds.}
    \label{tab:results}
\end{table}



\subsection{Discussion}
% TODO
The majority of results obtained from our solver matched those of MiniSat, verifying its correctness.
In terms of performance, our solver performed comparably on smaller instances but showed increased
runtime for larger benchmarks. Potential reasons for this performance difference include heuristic
differences, implementation optimizations, and memory management strategies.
Future optimizations may focus on improving clause learning efficiency and branching heuristics to
further reduce solving time while maintaining correctness.

\section{Experimental Setup}
\label{sec:experiment}
% TODO
In this section, describe the experimental setup used to evaluate the solver. Include:
\begin{itemize}
    \item Description of the SAT instances used in the experiments (e.g., DIMACS benchmark formats, problem types).
    \item Hardware and software environment for running experiments (e.g., processor, memory, operating system, libraries).
    \item Metrics used for evaluating solver performance (e.g., runtime, number of conflicts, memory usage, number of clauses learned).
          % How did we setup the testing?
\end{itemize}

\section{Results and Evaluation}
\label{sec:results}
% TODO
This section should present the results of your experiments and analyze the performance of your solver. Topics to include:
\begin{itemize}
    \item Summary of experimental results with tables and figures (e.g., performance on different benchmarks, comparison with other solvers if applicable).
    \item Analysis of the solver’s strengths and weaknesses based on the results.
    \item Discussion on the impact of various optimizations and heuristics on solver performance.
          % How many of the tests passed? What was the biggest TIMEOUT needed?
          % Did the VSIDS heuristic help?
\end{itemize}

\section{Challenges and Lessons Learned}
\label{sec:challenges}

During the development and evaluation of our CDCL solver, we encountered several challenges that
required careful debugging, algorithmic refinements, and adaptations to external dependencies.
This section outlines the key difficulties faced and the lessons learned throughout the process.

One of the initial challenges was correctly following the CDCL algorithm as described in the
reference materials \cite{CdclAlgorithm}. The visual representations in the slides were
sometimes ambiguous, making it unclear what exact computations were being performed at certain
stages. A notable example was the selection of the next watched literal, which we initially
misunderstood due to implicit details, actually needs to \textit{loop back around} sometimes,
not explicitly illustrated in the reference diagrams.

Managing literals and their negations efficiently within the codebase also became a source of
confusion. The tracking logic became increasingly complex, especially when handling clause
propagation and backtracking. To address this, we refined our data structures and improved
the organization of literals within the solver, with the separate \textbf{Literal} struct,
making operations more transparent and reducing the likelihood of errors.

Debugging the solver proved to be another major challenge, particularly when implementing the
decision-making functionality. The need to test with \textit{determinism} a methods that uses
randomness led us to introduce the \texttt{DecideHeuristic} abstraction. This allowed us to
have both a standard version that made random assignments and a test-oriented version that could
be precisely controlled. For the latter, we used the \texttt{mockall} crate, enabling us to
define specific values for the \texttt{decide} function during unit testing, what allowed to
write and diagnose issues related to decision strategies.

For more complex debugging scenarios, especially those involving intricate edge cases, we had
to configure a Rust debugger using \textbf{LLDB} \cite{LLDB}. This setup allowed us to execute
both the solver binary and individual unit tests in a step-by-step manner, analyzing control flow,
and stepping into function calls, we were able to pinpoint the root causes of unexpected behavior
like infinite loops and subtle logical errors that were difficult to catch through standard logging.

Additionally, we encountered a problem with the \texttt{dimacs} crate \cite{DimacsCrate}, which
we used to parse input problems in the DIMACS format \cite{DimacsFormat}. The issue arose when
comments (lines starting with the character \texttt{c}) appeared in the middle of clause definitions,
leading to parsing failures.
% TODO: Check if this is going to work...
To resolve this, we had to fork the crate and extend its functionality to correctly handle comments
interspersed within clauses. Another edge case is where an empty clause was also causing the parser
to fail. Our modifications ensured that such cases were handled correctly, preventing unintended
crashes.

% TODO: Conclusion?
% These challenges reinforced the importance of precise algorithmic understanding, well-structured
% abstractions, and robust debugging tools. The lessons learned throughout this process not only
% improved our solver’s correctness and efficiency but also deepened our understanding of SAT
% solving techniques and Rust's debugging ecosystem.

\section{Conclusion and Future Work}
\label{sec:conclusion}
% TODO
In the conclusion, summarize the key findings and suggest potential improvements:
\begin{itemize}
    \item Recap of the objectives and how they were achieved.
    \item Summary of the solver’s performance and main takeaways.
    \item Suggestions for future work (e.g., improvements to heuristics, additional optimizations, new features to implement).
\end{itemize}

\begin{thebibliography}{00}
    \bibitem{CdclAlgorithm}         A. Oliveras, E. Rodriguez-Carbonell ``From DPLL to CDCL SAT solvers'', January of 2025, Available at \url{https://www.cs.upc.edu/~oliveras/LAI/cdcl.pdf}.
    \bibitem{RustLang}              ``The Rust Reference'', January of 2025, Available at \url{https://doc.rust-lang.org/reference/index.html}.
    \bibitem{Carcara}               B. Andreotti, H. Lachnitt, H. Barbosa ``Carcara: An Efficient Proof Checker and Elaborator for SMT Proofs in the Alethe Format'', April of 2023, Available at \url{https://link.springer.com/chapter/10.1007/978-3-031-30823-9\_19}.
    \bibitem{SatBenchmarks}         ``SATLIB - Benchmark Problems'', January of 2025, Available at \url{https://www.cs.ubc.ca/~hoos/SATLIB/benchm.html}.
    \bibitem{AbcdclGithub}          B. Borges, A. Carvalho ``Atila - Bernardo - CDCL SAT Solver'', January of 2025, Available at \url{https://github.com/bernborgess/abcdcl}.
    \bibitem{Minisat}               N. Eén, N. Sörensson ``The MiniSat Page'', January of 2025, Available at \url{http://minisat.se/MiniSat.html}.
    \bibitem{VSIDS}                 M. Moskewicz, C. Madigan, Y. Zhao, L. Zhang, S. Malik ``Chaff: Engineering an Efficient SAT Solver'', January of 2025, Available at \url{https://www.princeton.edu/~chaff/publication/DAC2001v56.pdf}.
    \bibitem{LLDB}                  ``The LLDB Debugger'', January of 2025, Available at \url{https://lldb.llvm.org}.
    \bibitem{DimacsCrate}           ``Crate dimacs - The parser facility for parsing .cnf and .sat files as specified in the DIMACS format specification'', January of 2025, Available at \url{https://docs.rs/dimacs/0.2.0/dimacs/index.html}.
    \bibitem{DimacsFormat}          ``DIMACS CNF Format'', January of 2025, Available at \url{https://satcompetition.github.io//2009/format-benchmarks2009.html}.
\end{thebibliography}

\newpage
\appendix
\section{Appendix: Source Code}
\label{sec:appendix}
% TODO or REMOVE
Include the relevant source code snippets or provide a link to the full implementation if it's hosted in a public repository (e.g., GitHub). For code snippets:
\begin{minted}[frame=single, linenos=true, breaklines=true, fontsize=\footnotesize]{rust}
fn main() {
    println!("Hello, CDCL Solver!");
    // Your implementation here
}
\end{minted}

\end{document}
