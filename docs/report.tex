\documentclass[a4paper,12pt]{article}

% Include packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{minted}

% Set the page margins
\geometry{top=1in, bottom=1in, left=1in, right=1in}

% Fancy header
\pagestyle{fancy}
\setlength{\headheight}{14.49998pt}
\fancyhead[L]{CDCL Solver Report}
\fancyhead[R]{Date}

\title{ABCDCL - an implementation of CDCL with two watched literals in Rust\\
{\footnotesize Course Project of DCC831 Theory and Practice of SMT Solving}
}
\author{Atila Carvalho \& Bernardo Borges}
\date{January 27, 2025}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}

This report presents our course project for \textit{DCC831 - Theory and Practice of SMT Solving},
where we implemented and evaluated a modern SAT solver based on the Conflict-Driven Clause Learning
(CDCL) algorithm \cite{CdclAlgorithm}. Through this implementation, we explored both the theoretical
foundations and practical challenges of building an efficient SAT solver, gaining valuable insights
into the complexities of state-of-the-art approaches.

The CDCL algorithm incorporates several key concepts covered in the course, such as unit propagation,
clause learning, and non-chronological backtracking. Understanding these principles was essential not
only for implementing the solver but also for debugging and refining its performance. Throughout the
development process, we wrote extensive tests to verify correctness and experimented with different
heuristics to optimize decision-making.

We chose Rust \cite{RustLang} as our implementation language due to its strong emphasis on memory
safety, performance, and modern software engineering practices. Additionally, Rust is the primary
language used in \textit{Carcara} \cite{Carcara}, a proof checker we have been working with, making
it a natural choice for this project.

To evaluate our solver, we developed scripts that automatically download SAT benchmarks \cite{SatBenchmarks}
in the DIMACS format and run them against both our implementation and the widely used \textit{MiniSat}
solver \cite{Minisat}. The results are logged and compared, providing an empirical analysis of our
solver's performance and correctness.

\section{Solver Design and Implementation}
\label{sec:design}
% TODO
This section should describe the architecture and design choices of your CDCL solver. Some topics to cover:
\begin{itemize}
    \item Description of the CDCL algorithm: search process, conflict analysis, backtracking, clause learning.
    \item Key data structures used in the solver (e.g., decision stack, implication graph, learned clauses).
    \item Heuristics implemented (e.g., variable selection, decision ordering).
    \item Optimizations applied (e.g., restarts, watched literals, UIP learning).
\end{itemize}

\section{Testing and Evaluation}

For the evaluation of our CDCL solver, we used a set of benchmark problems in DIMACS
format from the SATLIB repository. Our solver was compared against MiniSat, a
well-established SAT solver, in terms of correctness and runtime.

\subsection{Experimental Setup}
All experiments were conducted on a laptop with the following hardware and software configuration:

\begin{itemize}
    \item \textbf{Operating System:} Arch Linux (Kernel 6.12.10)
    \item \textbf{Host:} ASUS VivoBook X512FJ
    \item \textbf{Processor:} Intel Core i7-8565U (8 threads, up to 4.60GHz)
    \item \textbf{RAM:} 8GB (usable: 7.8GB)
    \item \textbf{GPU:} Intel UHD Graphics 620 (Whiskey Lake) + NVIDIA GeForce MX230
\end{itemize}

\subsection{Results}
For each benchmark instance, we recorded the solver’s result (SAT or UNSAT) and its
execution time in milliseconds. The results were then compared against MiniSat to
verify correctness and measure performance. The summarized results are presented in
Table~\ref{tab:results}, comparing Avg Time to solve in \textit{ms} for each benchmark
and it's correctness overall.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        \textbf{Benchmark} & \textbf{ABCDCL Time} & \textbf{MiniSat Time} & \textbf{Results } \\
        \hline
        % TODO
        jnh                & 397                  & 0                     & 50 / 0 / 0 / 50   \\
        jnh                & 492                  & 2                     & 50 / 0 / 0 / 50   \\

        \hline
    \end{tabular}
    \caption{Performance comparison of ABCDCL and MiniSat across benchmark families. Results is a
        counter of \texttt{(PASS/FAIL/TIME/TOTAL)}. The timeout used was $60$ seconds.}
    \label{tab:results}
\end{table}



\subsection{Discussion}
% TODO
The majority of results obtained from our solver matched those of MiniSat, verifying its correctness.
In terms of performance, our solver performed comparably on smaller instances but showed increased
runtime for larger benchmarks. Potential reasons for this performance difference include heuristic
differences, implementation optimizations, and memory management strategies.
Future optimizations may focus on improving clause learning efficiency and branching heuristics to
further reduce solving time while maintaining correctness.


\section{Challenges and Lessons Learned}
\label{sec:challenges}

During the development and evaluation of our CDCL solver, we encountered several challenges that
required careful debugging, algorithmic refinements, and adaptations to external dependencies.
This section outlines the key difficulties faced and the lessons learned throughout the process.

One of the initial challenges was correctly following the CDCL algorithm as described in the
reference materials \cite{CdclAlgorithm}. The visual representations in the slides were
sometimes ambiguous, making it unclear what exact computations were being performed at certain
stages. A notable example was the selection of the next watched literal, which we initially
misunderstood due to implicit details, actually needs to \textit{loop back around} sometimes,
not explicitly illustrated in the reference diagrams.

Managing literals and their negations efficiently within the codebase also became a source of
confusion. The tracking logic became increasingly complex, especially when handling clause
propagation and backtracking. To address this, we refined our data structures and improved
the organization of literals within the solver, with the separate \textbf{Literal} struct,
making operations more transparent and reducing the likelihood of errors.

Debugging the solver proved to be another major challenge, particularly when implementing the
decision-making functionality. The need to test with \textit{determinism} a methods that uses
randomness led us to introduce the \texttt{DecideHeuristic} abstraction. This allowed us to
have both a standard version that made random assignments and a test-oriented version that could
be precisely controlled. For the latter, we used the \texttt{mockall} crate, enabling us to
define specific values for the \texttt{decide} function during unit testing, what allowed to
write and diagnose issues related to decision strategies.

For more complex debugging scenarios, especially those involving intricate edge cases, we had
to configure a Rust debugger using \textbf{LLDB} \cite{LLDB}. This setup allowed us to execute
both the solver binary and individual unit tests in a step-by-step manner, analyzing control flow,
and stepping into function calls, we were able to pinpoint the root causes of unexpected behavior
like infinite loops and subtle logical errors that were difficult to catch through standard logging.

Additionally, we encountered a problem with the \texttt{dimacs} crate \cite{DimacsCrate}, which
we used to parse input problems in the DIMACS format \cite{DimacsFormat}. The issue arose when
comments (lines starting with the character \texttt{c}) appeared in the middle of clause definitions,
leading to parsing failures. Another edge case involved empty clauses, which also caused the parser to fail.

Ideally, we would have forked the crate and extended its functionality to correctly handle these cases.
However, due to time constraints, we were unable to implement these fixes. As a result, certain test cases,
namely \texttt{sat10.cnf}, \texttt{sat12.cnf} and \texttt{false.cnf}, are expected to fail due to parse errors.

Furthermore, we are currently working on implementing the VSIDS decision heuristic to improve solver performance.
Unfortunately, this feature will not be ready in time for this report, but we plan to update the
repository once it is completed.

\section{Conclusion and Future Work}
\label{sec:conclusion}
This project proved to be significantly more challenging than anticipated, primarily due to the nuanced
balance between correctness and efficiency. Through the process, we gained a deep understanding of both
the CDCL algorithm and the intricacies of implementing it in Rust. Despite the pressure, we enjoyed developing
creative solutions, such as implementing unit tests, mocks, and scripts, comparing our solver with an
existing one \cite{Minisat}, and overcoming challenges with the parser crate. We were heavily inspired
by a Python implementation \cite{PythonCDCL}, and in understanding its code, we forked and enhanced it
by adding strict typing, which was later merged into the original repository \url{https://github.com/Kienyew/CDCL-SAT-Solver-from-Scratch/pull/1}.

For future work, we plan to further refine the solver by implementing the VSIDS decision heuristic,
which is expected to enhance performance. We also aim to fix the issues related to the DIMACS parser,
ensuring that the solver can handle all edge cases correctly. Overall, this project was a rewarding
challenge that deepened our understanding of both SAT solving and Rust programming.

\begin{thebibliography}{00}
    \bibitem{CdclAlgorithm}         A. Oliveras, E. Rodriguez-Carbonell ``From DPLL to CDCL SAT solvers'', January of 2025, Available at \url{https://www.cs.upc.edu/~oliveras/LAI/cdcl.pdf}.
    \bibitem{RustLang}              ``The Rust Reference'', January of 2025, Available at \url{https://doc.rust-lang.org/reference/index.html}.
    \bibitem{Carcara}               B. Andreotti, H. Lachnitt, H. Barbosa ``Carcara: An Efficient Proof Checker and Elaborator for SMT Proofs in the Alethe Format'', April of 2023, Available at \url{https://link.springer.com/chapter/10.1007/978-3-031-30823-9\_19}.
    \bibitem{SatBenchmarks}         ``SATLIB - Benchmark Problems'', January of 2025, Available at \url{https://www.cs.ubc.ca/~hoos/SATLIB/benchm.html}.
    \bibitem{AbcdclGithub}          B. Borges, A. Carvalho ``Atila - Bernardo - CDCL SAT Solver'', January of 2025, Available at \url{https://github.com/bernborgess/abcdcl}.
    \bibitem{Minisat}               N. Eén, N. Sörensson ``The MiniSat Page'', January of 2025, Available at \url{http://minisat.se/MiniSat.html}.
    \bibitem{VSIDS}                 M. Moskewicz, C. Madigan, Y. Zhao, L. Zhang, S. Malik ``Chaff: Engineering an Efficient SAT Solver'', January of 2025, Available at \url{https://www.princeton.edu/~chaff/publication/DAC2001v56.pdf}.
    \bibitem{LLDB}                  ``The LLDB Debugger'', January of 2025, Available at \url{https://lldb.llvm.org}.
    \bibitem{DimacsCrate}           ``Crate dimacs - The parser facility for parsing .cnf and .sat files as specified in the DIMACS format specification'', January of 2025, Available at \url{https://docs.rs/dimacs/0.2.0/dimacs/index.html}.
    \bibitem{DimacsFormat}          ``DIMACS CNF Format'', January of 2025, Available at \url{https://satcompetition.github.io//2009/format-benchmarks2009.html}.
    \bibitem{PythonCDCL}            ``CDCL SAT Solver from Scratch in Python'', January of 2025, Available at \url{https://github.com/Kienyew/CDCL-SAT-Solver-from-Scratch}.
\end{thebibliography}

\end{document}
